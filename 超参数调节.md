# GRU超参数调节记录

##基本设置

- 迭代次数：50

  RNN(
    (rnn): GRU(66, 256, num_layers=2, batch_first=True, dropout=0.5)
    (out): Linear(in_features=256, out_features=66, bias=True)
  )

指标是最后3次在训练集上和验证集上的loss。

## 1. 有无mask

输入参数中是否应该包含mask？

- > 0.00026 0.00460 [包括]
  >

  0.00022 0.00442

  0.00021 0.00450 

- 0.00021 0.00404 [不包括]
  0.00020 0.00410
  0.00020 0.00407

结论：有mask会导致输入参数过多，训练难度增加啊，因而loss增加。

取消mask。

###2. LOSS设置

在loss中，为缺失部分增加权重。

参数weight表示，缺失部分所占的额外权重。

- weight=1

> 0.00012 0.00250
> 0.00011 0.00246
> 0.00011 0.00242     

- weight=0.5

> 0.00014  0.00317<br/>0.00016  0.00314
> 0.00016  0.00312

- weight=0

> 0.00026  0.00460<br/>0.00022  0.00442
> 0.00021  0.00450

结论，weight 也许应大些。

设为1。

## 3. 增加GRU层数

时间太长，故将迭代次数调整为20。

- 1层

> 0.00001 0.00018
> 0.00001 0.00018
> 0.00001 0.00017

- 2层

> 0.00017 0.00170
> 0.00014 0.00177
> 0.00013 0.00179

- 3层

> 0.00020 0.00322
> 0.00018 0.00332
> 0.00019 0.00339

- 4层

> 0.00031 0.00400
> 0.00026 0.00420
> 0.00029 0.00408

结论，保持2层。

### 4. dropout

迭代次数为 25

- dropout=0.5

> 0.00017 0.00170
> 0.00014 0.00177
> 0.00013 0.00179

- dropout=0.2

> 0.00007 0.00065
> 0.00005 0.00059
> 0.00008 0.00053

- dropout=0.1 

> 126.8s
>
> 0.00008 0.00048
> 0.00005 0.00045
> 0.00004 0.00047

- dropout=0

> 0.00001 0.00022
> 0.00001 0.00021
> 0.00001 0.00022

结论：dropout取较大的值，能够防止过拟合，但同时收敛的速度变慢。

dropout=0.05。